{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sadickachuli/data_preprocessing/blob/main/ml_pipeline_%5BAchuli_Mustapha_Sadick%5D_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37BV7-CMEbMj"
      },
      "source": [
        "# Data Processing Approach for Portfolio Project\n",
        "\n",
        "## Project Title: Garbage Classification for Recyclable and Non-Recyclable Waste\n",
        "## [Company Logo]\n",
        "\n",
        "## Student Name: Achuli Mustapha Sadick\n",
        "\n",
        "---\n",
        "\n",
        "1. **Data Sources and Aggregation:**\n",
        "   - List all sources of data for the project. **You must consider sources outside kaggle, google datasets** (insert links where necessary to online platforms,research papers etc)\n",
        "\n",
        "   **I used kaggle and IEEE for dataset searching and chose this dataset (https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification/data)**\n",
        "   \n",
        "   - Determine if data aggregation from multiple sources is necessary for comprehensive analysis.\n",
        "\n",
        "   **- The dataset includes multiple classes like \"cardboard,\" \"metal,\" \"plastic,\" etc., for recyclable materials, and only one class, \"trash,\" for non-recyclable items.**\n",
        "\n",
        "  **- To address class imbalance and enhance model accuracy, additional datasets may be aggregated, particularly for the \"trash\" class. This could involve combining different datasets that focus on household waste or non-recyclable waste.**\n",
        "\n"
      ],
      "id": "37BV7-CMEbMj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing essential libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Sklearn for evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Drive mounted')\n",
        "\n",
        "# The base directory\n",
        "base_dir = '/content/drive/MyDrive/Garbage_Classification_Data_Split'\n",
        "\n",
        "# The input data directory\n",
        "input_data_dir = '/content/drive/MyDrive/Garbage_classification_data'"
      ],
      "metadata": {
        "id": "K6W_BkY6FGRL"
      },
      "id": "K6W_BkY6FGRL",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2. **Data Format Transformation:**\n",
        "   - Describe the current format of the data.\n",
        "   - Outline the planned transformation to a unified format suitable for analysis and modeling.\n",
        "\n",
        " **- My dataset includes images in jpg and a label in csv format.**\n",
        "\n",
        " **- Resize images to a standard resolution (e.g., 128x128) to improve training consistency.**\n",
        "\n",
        " **- Data augmentation can be applied to improve generalization, especially for classes with fewer samples.**\n",
        "\n",
        "3. **Data Exploration:**\n",
        "   - Enumerate the features included in the dataset.\n",
        "   \n",
        "   - Summarize findings from exploratory data analysis (EDA) including distributions, correlations, and outliers.\n",
        "   \n",
        "  **Insert code for data exploration below**\n"
      ],
      "metadata": {
        "id": "TzyeghJDEjit"
      },
      "id": "TzyeghJDEjit"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Load and Preprocess the Dataset\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# ImageDataGenerator for training with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "6ayPy75-8Ipo"
      },
      "id": "6ayPy75-8Ipo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Include plots for EDA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Plot class distribution\n",
        "class_counts = {class_name: len(os.listdir(f\"input_data_dir/{class_name}\")) for class_name in class_names}\n",
        "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oYEgIOxSFUHW"
      },
      "id": "oYEgIOxSFUHW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "4. **Hypothesis Testing:**\n",
        "   - State any preexisting hypotheses about the data.\n",
        "   - Explain methodologies to empirically test these hypotheses.\n",
        "**1. Hypothesis:**\n",
        "\n",
        "   **- Hypothesis 1: The model may face difficulty distinguishing between similar-looking classes like \"metal\" and \"cardboard\" on a similar-colored background.**\n",
        "\n",
        "   **- Hypothesis 2: Adding more samples to the \"trash\" class (non-recyclable) will improve model accuracy by reducing bias toward recyclable materials.**\n",
        "\n",
        "**2. Testing Methodologies:**\n",
        "\n",
        "   **- Evaluate the model on a subset of images containing similar backgrounds and observe the modelâ€™s performance.**\n",
        "\n",
        "   **- Perform additional training with balanced class samples for the \"trash\" category and compare the results.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**5. Handling Sparse/Dense Data and Outliers:**\n",
        "   - Assess the density of the data.\n",
        "   - Propose strategies to handle missing data and outliers while maintaining dataset integrity.\n",
        "\n",
        "   **Insert code for Handling Sparse/Dense Data and Outliers below**"
      ],
      "metadata": {
        "id": "tHKtxjFKFNpB"
      },
      "id": "tHKtxjFKFNpB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing corrupted or non-image files\n",
        "def filter_invalid_images(file_list):\n",
        "    valid_files = []\n",
        "    for file in file_list:\n",
        "        try:\n",
        "            img = plt.imread(file)\n",
        "            valid_files.append(file)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return valid_files"
      ],
      "metadata": {
        "id": "f_R52s5RE_wj"
      },
      "id": "f_R52s5RE_wj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Data Splitting:**\n",
        "   - Define a methodology to split the dataset into training, validation, and testing sets.\n",
        "   - Ensure randomness and representativeness in each subset.\n",
        "\n",
        "7. **Bias Mitigation:**\n",
        "   - Implement techniques to identify and mitigate biases in the dataset.\n",
        "   - Ensure fairness and equity in data representation.\n",
        "   \n",
        "    **The split method (e.g., 70% training, 15% validation, 15% testing) to ensure balanced representation across classe**\n",
        "\n"
      ],
      "metadata": {
        "id": "5SS3lD0sGPGe"
      },
      "id": "5SS3lD0sGPGe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Class Names\n",
        "class_names = os.listdir(input_data_dir)\n",
        "class_names = [class_name for class_name in class_names\n",
        "               if os.path.isdir(os.path.join(input_data_dir, class_name))\n",
        "               and class_name not in ['train', 'validation', 'test']]\n",
        "print(f\"Classes found: {class_names}\")\n",
        "\n",
        "# Split Data into Train, Validation, and Test\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(input_data_dir, class_name)\n",
        "    files = os.listdir(class_dir)\n",
        "\n",
        "    # Filter out non-file entries\n",
        "    files = [f for f in files if os.path.isfile(os.path.join(class_dir, f))]\n",
        "\n",
        "    if not files:\n",
        "        print(f\"No files found in class '{class_name}'. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Shuffle files\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    np.random.shuffle(files)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total = len(files)\n",
        "    train_size = int(0.7 * total)\n",
        "    val_size = int(0.15 * total)\n",
        "    test_size = total - train_size - val_size\n",
        "\n",
        "        # Split files\n",
        "    train_files = files[:train_size]\n",
        "    val_files = files[train_size:train_size + val_size]\n",
        "    test_files = files[train_size + val_size:]\n",
        "\n",
        "    # Create class directories in train, validation, and test folders\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Function to copy files safely\n",
        "    def copy_files(file_list, src_dir, dest_dir, class_name, split_type):\n",
        "        for file in file_list:\n",
        "            src_file = os.path.join(src_dir, file)\n",
        "            dest_file = os.path.join(dest_dir, class_name, file)\n",
        "            if os.path.isfile(src_file):\n",
        "                try:\n",
        "                    shutil.copy(src_file, dest_file)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {src_file} to {dest_file}: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping {src_file}, not a file.\")\n",
        "\n",
        "    # Moving files to respective directories\n",
        "    copy_files(train_files, class_dir, train_dir, class_name, 'train')\n",
        "    copy_files(val_files, class_dir, val_dir, class_name, 'validation')\n",
        "    copy_files(test_files, class_dir, test_dir, class_name, 'test')\n",
        "\n",
        "    print(f\"Class '{class_name}' split into Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
        "\n",
        "print(\"Data split completed.\")"
      ],
      "metadata": {
        "id": "iX_5Hl65GEgY"
      },
      "id": "iX_5Hl65GEgY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Features for Model Training:**\n",
        "   - Identify relevant features for training the model.\n",
        "   - Rank features based on their significance to project objectives.\n",
        "\n",
        " **Your answer for features must be plotted/ show your working code-wise **\n",
        "9. **Types of Data Handling:**\n",
        "   - Classify the types of data (categorical, numerical, etc.) present in the dataset.\n",
        "   - Plan preprocessing steps for each data type.\n",
        "\n",
        "   [Types of Data:\n",
        "\n",
        "Categorical Data: Image data with categorical labels (recyclable, non-recyclable).\n",
        "Numerical Data: Not applicable, as the data is image-based.\n",
        "Preprocessing:\n",
        "\n",
        "Images: Rescale, resize, and augment image data.\n",
        "Labels: One-hot encode class labels if necessary for classification.s]\n"
      ],
      "metadata": {
        "id": "OzGrkCCXGUxV"
      },
      "id": "OzGrkCCXGUxV"
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of images to be displayed for each class\n",
        "num_images_to_display = 2\n",
        "\n",
        "# Function to display images from each class\n",
        "def display_images_from_classes(base_dir, class_names, num_images):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        images = os.listdir(class_dir)\n",
        "\n",
        "        # Shuffle images for random display\n",
        "        np.random.shuffle(images)\n",
        "\n",
        "        # Display images for each class\n",
        "        for j in range(min(num_images, len(images))):\n",
        "            img_path = os.path.join(class_dir, images[j])\n",
        "            img = mpimg.imread(img_path)\n",
        "            plt.subplot(len(class_names), num_images, i * num_images + j + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(class_name)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YNkqADAQGbNZ"
      },
      "id": "YNkqADAQGbNZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "10. **Data Transformation for Modeling:**\n",
        "    - Specify methods for transforming raw data into a model-friendly format.\n",
        "    - Detail steps for normalization, scaling, or encoding categorical variables.\n",
        "\n",
        "    ***Raw Data Transformation:***\n",
        "\n",
        "- Image Transformation: Resize, normalize, and augment images to prepare them for model training.\n",
        "\n",
        "11. **Data Storage:**\n",
        "    - Determine where and how processed data will be stored.\n",
        "    - Choose suitable storage solutions ensuring accessibility and security.\n",
        "\n",
        "    ***Data Storage:***\n",
        "    \n",
        "Storage Solutions:\n",
        "\n",
        "- Store processed images in the local filesystem or cloud storage (e.g., AWS S3) for easy access during training.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Notes:\n",
        "- This template provides a structured framework for documenting your data processing approach for the portfolio project.\n",
        "- Fill out each section with specific details relevant to your project's requirements and objectives.\n",
        "- Use additional cells as needed to provide comprehensive information."
      ],
      "metadata": {
        "id": "lG_mx92GGX6W"
      },
      "id": "lG_mx92GGX6W"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itbA2woJqZBw"
      },
      "id": "itbA2woJqZBw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}